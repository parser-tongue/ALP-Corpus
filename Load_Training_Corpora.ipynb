{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import emoji\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import xml.etree.ElementTree as et\n",
    "from collections import defaultdict\n",
    "from tokenize_GSW import tokenize\n",
    "from normalizerFunctions import Training_Corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ArchiMob data \n",
    "def parse_ArchiMob(folder_path):\n",
    "    # returns a list of (word, norm) pairs\n",
    "    utterances = []\n",
    "    for xml_file_path in os.listdir(folder_path):\n",
    "        if xml_file_path == \"Metadata.txt\" or xml_file_path == \"person_file.xml\":\n",
    "            continue\n",
    "        tree = et.parse(folder_path+xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        for u_element in root.iter('{http://www.tei-c.org/ns/1.0}u'):\n",
    "            utterance = []\n",
    "            for w_element in u_element.iter('{http://www.tei-c.org/ns/1.0}w'):\n",
    "                word = w_element.text\n",
    "                if word:\n",
    "                    if word[-3:] == \"***\":            # ignores individuals' names hidden with asterisks in the ArchiMob corpus\n",
    "                        continue\n",
    "                    norm = w_element.get('normalised')\n",
    "                    utterance.append((word, norm))\n",
    "            utterances.append(utterance)\n",
    "    return utterances\n",
    "    \n",
    "# read WUS dataset and update the normalization dictionary  \n",
    "def parse_WUS(folder_path):\n",
    "    # returns a list of (word, norm) pairs\n",
    "    word_norm_pairs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with open(os.path.join(folder_path, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "            reader = csv.reader(file, delimiter=\"\\t\")\n",
    "            i = 0 \n",
    "            sent = []\n",
    "            for row in reader:\n",
    "                word= row[0]\n",
    "                try:\n",
    "                    norm = row[1]\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                if emoji.is_emoji(word[0]):\n",
    "                    continue\n",
    "                word = word.strip(string.punctuation)\n",
    "                norm = norm.strip(string.punctuation)\n",
    "                if word:\n",
    "                    word = word.lstrip()\n",
    "                    if \" \" in word:     # handles many-to-one normalizations\n",
    "                        word = word.replace(\" \",\"-\") \n",
    "                    sent.append((word, norm)) \n",
    "                else: \n",
    "                    word_norm_pairs.append(sent) \n",
    "                    sent = []   \n",
    "    return word_norm_pairs\n",
    "\n",
    "# expand the corpus with additional normalizations from the Bilingual Lexicon\n",
    "def parse_Bilexicon(file_path):\n",
    "    # returns a list of (word, norm) pairs\n",
    "    with open(file_path, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        norms = {}\n",
    "        next(csv_reader)  \n",
    "        for row in csv_reader:\n",
    "            key = row[1] \n",
    "            key = key.strip(string.punctuation)\n",
    "            value = row[2] \n",
    "            value = value.strip(string.punctuation)\n",
    "            norms[key] = value\n",
    "        return [norms.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_replacement(dictionary, word):\n",
    "    max_value = max(dictionary.values())  \n",
    "    max_value_keys = [k for k, v in dictionary.items() if v == max_value]\n",
    "    return random.choice(max_value_keys)\n",
    "\n",
    "def leave_as_is(dictionary, word):\n",
    "    return word\n",
    "\n",
    "\n",
    "def get_norms_and_error_rate(list_of_utterances, counts, method):\n",
    "    normed_utterances = []\n",
    "    total = 0 \n",
    "    hits = 0\n",
    "    words_unnormed = 0\n",
    "    for u in list_of_utterances:\n",
    "        normed_utterance = []\n",
    "        for (word, norm) in u:\n",
    "            if word == norm:\n",
    "                words_unnormed += 1\n",
    "            prediction = method(counts[word], word) #TODO: only picks a random norm when max freq is tied\n",
    "            normed_utterance.append(prediction)\n",
    "            if prediction == norm:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "        normed_utterances.append(normed_utterance)\n",
    "    accuracy = 100*hits/total\n",
    "    unnormed = 100*words_unnormed/total\n",
    "    print(accuracy)\n",
    "    print(unnormed)\n",
    "    #Rob van der Goot. 2019b. Normalization and Parsing Algorithms for Uncertain Input. Ph.D. thesis, Uni- versity of Groningen.\n",
    "    Err_Red_rate = (accuracy - unnormed)/(100 - unnormed) # all are percentages\n",
    "    return normed_utterances, Err_Red_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "archimob = 'Archimob_Release_2/'\n",
    "wus = \"WUS/\"\n",
    "bilexicon = 'bilexicon.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_wus = Training_Corpus(wus, parse_WUS)\n",
    "c_archimob = Training_Corpus(archimob, parse_ArchiMob)\n",
    "c_bilexicon = Training_Corpus(bilexicon, parse_Bilexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens:  93674\n",
      "total unique wordforms:  9052\n",
      "total normalized forms:  6907\n",
      " \n",
      "total tokens:  581466\n",
      "total unique wordforms:  48539\n",
      "total normalized forms:  31696\n",
      " \n",
      "total tokens:  137051\n",
      "total unique wordforms:  137051\n",
      "total normalized forms:  94896\n",
      " \n"
     ]
    }
   ],
   "source": [
    "corpora = [c_wus,c_archimob, c_bilexicon]\n",
    "for c in corpora:\n",
    "    print(\"total tokens: \",sum([sum(count.values()) for _, count in c.norm_dict.items()]))\n",
    "    print(\"total unique wordforms: \",len(c.norm_dict))\n",
    "    labels = set([norm for _,norm_dict in c.norm_dict.items() for norm in norm_dict.keys()])\n",
    "    print(\"total normalized forms: \", len(labels))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to calculate the error reduction rate of a normalization method \n",
    "\n",
    "def get_key_with_highest_value(dictionary):\n",
    "    max_value = max(dictionary.values())  \n",
    "    max_value_keys = [k for k, v in dictionary.items() if v == max_value]\n",
    "    return random.choice(max_value_keys)  # when max freq is tied, picks one of the best at random \n",
    "\n",
    "def get_norms_and_error_rate(list_of_utterances, counts, method):\n",
    "    normed_utterances = []\n",
    "    total = 0 \n",
    "    hits = 0\n",
    "    words_unnormed = 0\n",
    "    for u in list_of_utterances:\n",
    "        normed_utterance = []\n",
    "        for (word, norm) in u:\n",
    "            if word == norm:\n",
    "                words_unnormed += 1\n",
    "            prediction = method(counts[word])\n",
    "            normed_utterance.append(prediction)\n",
    "            if prediction == norm:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "        normed_utterances.append(normed_utterance)\n",
    "    accuracy = 100*hits/total\n",
    "    unnormed = 100*words_unnormed/total\n",
    "    print('MFR Corpus accuracy: \\t',accuracy)\n",
    "    print('MFR Proportion left: \\t',unnormed)\n",
    "    #Rob van der Goot. 2019b. Normalization and Parsing Algorithms for Uncertain Input. Ph.D. thesis, Uni- versity of Groningen.\n",
    "    Err_Red_rate = (accuracy - unnormed)/(100 - unnormed) # all are percentages\n",
    "    return normed_utterances, Err_Red_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFR Corpus accuracy: \t 91.54413155713318\n",
      "MFR Proportion left: \t 24.67590538397774\n",
      "ArchiMob ERR with MFR: \t 0.8877401914225178\n"
     ]
    }
   ],
   "source": [
    "archiMob_MFR_norms, archiMob_ERR_MFR = get_norms_and_error_rate(c_archimob.word_norm_pairs, c_archimob.norm_dict, get_key_with_highest_value)\n",
    "print('ArchiMob ERR with MFR: \\t',archiMob_ERR_MFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFR Corpus accuracy: \t 86.1274206289899\n",
      "MFR Proportion left: \t 35.17304695006085\n",
      "WUS ERR with MFR: \t 0.7860059941376016\n"
     ]
    }
   ],
   "source": [
    "wus_MFR_norms, wus_ERR_MFR = get_norms_and_error_rate(c_wus.word_norm_pairs, c_wus.norm_dict, get_key_with_highest_value)\n",
    "print('WUS ERR with MFR: \\t',wus_ERR_MFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_corpora(list_of_corpora):\n",
    "    joined_corpus = []\n",
    "    for corpus in list_of_corpora:\n",
    "        if len(corpus.word_norm_pairs) ==1:     # handles the bilexicon\n",
    "            for pair in list(corpus.word_norm_pairs)[0]:\n",
    "                if pair:\n",
    "                    joined_corpus.extend([[pair]])\n",
    "        else:\n",
    "            joined_corpus.extend(corpus.word_norm_pairs)\n",
    "    return joined_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens:  812191\n",
      "total unique wordforms:  181744\n",
      "total normalized forms:  121603\n"
     ]
    }
   ],
   "source": [
    "norm_corpus = Training_Corpus(corpora,join_corpora)\n",
    "print(\"total tokens: \",sum([sum(count.values()) for _, count in norm_corpus.norm_dict.items()]))\n",
    "print(\"total unique wordforms: \",len(norm_corpus.norm_dict))\n",
    "labels = set([norm for _,norm_dict in norm_corpus.norm_dict.items() for norm in norm_dict.keys()])\n",
    "print(\"total normalized forms: \", len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFR Corpus accuracy: \t 90.65478440416109\n",
      "MFR Proportion left: \t 21.726785940745465\n",
      "Joined ERR with MFR: \t 0.8806077441924849\n"
     ]
    }
   ],
   "source": [
    "total_MFR_norms, total_ERR_MFR = get_norms_and_error_rate(norm_corpus.word_norm_pairs, norm_corpus.norm_dict, get_key_with_highest_value)\n",
    "print('Joined ERR with MFR: \\t',total_ERR_MFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus, filename in [(c_wus,\"pickled_wus.pkl\"),\n",
    "                         (c_archimob,\"pickled_archimob.pkl\"), \n",
    "                         (norm_corpus,'pickled_train_corpus.pkl')]:\n",
    "    with open(filename, \"wb\") as file:\n",
    "        pickle.dump(corpus, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
