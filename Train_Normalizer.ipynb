{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import time\n",
    "import datetime\n",
    "from normalizerFunctions import Training_Corpus\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from Normalization_Dataset import Normalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled_wus.pkl', 'rb') as file:\n",
    "    wus_corpus = pickle.load(file)\n",
    "with open('pickled_archimob.pkl', 'rb') as file:\n",
    "    archimob_corpus = pickle.load(file)\n",
    "with open('pickled_train_corpus.pkl', 'rb') as file:\n",
    "    norm_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sequence padding \n",
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    labels = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "    f = torch.LongTensor\n",
    "    return words, f(x), is_heads, labels, f(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Token_Classifier model class\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "class Token_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-swiss-lm')\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size) # this is just the ouput shape for BertModel\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.to(device) # (N, L). int64\n",
    "        y = y.to(device) # (N, L). int64\n",
    "        with torch.set_grad_enabled(self.training):\n",
    "            self.bert.train(self.training)\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat\n",
    "    \n",
    "    def normalize(self, x):\n",
    "        self.bert.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        logits = self.fc(enc)\n",
    "        prediction = logits.argmax(-1)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Dataloader(corpus, name):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-swiss-lm', do_lower_case=True, never_split=corpus.multigrams)\n",
    "    labels = [\"<pad>\"] + list(corpus.labels)\n",
    "    label2idx = {label:idx for idx, label in enumerate(labels)}\n",
    "    idx2label = {idx:label for idx, label in enumerate(labels)}\n",
    "    # create training / validation split and load data into batches \n",
    "    train_data, val_data = train_test_split(corpus.word_norm_pairs)\n",
    "    train_dataset = Normalization(train_data, label2idx, corpus.multigrams)\n",
    "    val_dataset = Normalization(val_data,label2idx, corpus.multigrams)\n",
    "\n",
    "    train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=8,\n",
    "                                shuffle=True,\n",
    "                                num_workers=0,\n",
    "                                collate_fn=pad)\n",
    "    val_iter = data.DataLoader(dataset=val_dataset,\n",
    "                                batch_size=8,\n",
    "                                shuffle=False,\n",
    "                                num_workers=0,\n",
    "                                collate_fn=pad)\n",
    "    with open(\"idx2label_\"+name+\".pickle\", \"wb\") as file:\n",
    "        pickle.dump(idx2label, file)\n",
    "    with open(\"label2idx_\"+name+\".pickle\", \"wb\") as file:\n",
    "        pickle.dump(label2idx, file)\n",
    "    print(\"Data loaded\")\n",
    "    return train_iter, val_iter, label2idx, idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalizer(corpus,name):\n",
    "    train_iter, val_iter, label2idx,idx2label = get_Dataloader(corpus,name)\n",
    "    model = Token_Classifier(vocab_size=len(label2idx))\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(True)\n",
    "        model = nn.DataParallel(model)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "    epochs = 3\n",
    "    # performance and monitoring metrics \n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "    total_steps = len(train_iter) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    # Full training loop\n",
    "    for epoch in range(0,epochs):\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "        print('Training...')\n",
    "        t0 = time.time()\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            # Progress update every 500 batches.\n",
    "            if step % 500 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('Batch {} of {}.Elapsed: {:}.'.format(step, len(train_iter), elapsed))\n",
    "            words, x, is_heads, labels, y, seqlens = batch\n",
    "            _y = y # for monitoring\n",
    "            optimizer.zero_grad()\n",
    "            logits, y, _ = model(x, y) # logits: (N, L, VOCAB), y: (N, L)\n",
    "            logits = logits.view(-1, logits.shape[-1]) # (N*L, VOCAB)\n",
    "            y = y.view(-1)  # (N*L,)\n",
    "            loss = criterion(logits, y)\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if step%1000==0: # monitoring\n",
    "                print(\"step: {}, loss: {}\".format(step, loss.item()))\n",
    "        avg_train_loss = total_train_loss / len(train_iter)\n",
    "        training_time = format_time(time.time() - t0)\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "        # Validation: \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "        t0 = time.time()\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        total = 0\n",
    "        hits = 0\n",
    "        words_unnormed = 0\n",
    "        with open(\"norm_test_\"+name+\".txt\", 'w') as fout: # generates a results file with the word, the true label, and the prediction\n",
    "            for batch in val_iter:\n",
    "                with torch.no_grad():        \n",
    "                    b_utterances, x, b_is_heads, b_labels, y, seqlens = batch\n",
    "                    _, _, b_predictions = model(x, y) # logits: (N, L, VOCAB), y: (N, L)\n",
    "                    b_predictions = b_predictions.detach().cpu().numpy() # pred_ids.cpu().numpy().tolist() alternative?\n",
    "                    assert len(b_utterances)==len(b_labels)== len(b_predictions)                \n",
    "                    for utterance, utterance_labels, utterance_preds, is_heads in zip(b_utterances, b_labels, b_predictions, b_is_heads):\n",
    "                        utterance_preds = [pred for head, pred in zip(is_heads, utterance_preds) if head == 1]\n",
    "                        for pred in utterance_preds:\n",
    "                            try:\n",
    "                                test = idx2label[pred]\n",
    "                            except KeyError:\n",
    "                                idx2label[pred] = '<pad>'\n",
    "                        preds = [idx2label[pred] for pred in utterance_preds]\n",
    "                        words = utterance.split()\n",
    "                        labels = utterance_labels.split()\n",
    "                        assert len(preds)==len(words)== len(labels)\n",
    "                        for w, l, p in zip(words[1:-1], labels[1:-1], preds[1:-1]):\n",
    "                            if w == l:\n",
    "                                words_unnormed += 1\n",
    "                            if l == p:\n",
    "                                hits += 1\n",
    "                            total += 1\n",
    "                            fout.write(\"{} {} {}\\n\".format(w, l, p))\n",
    "                        fout.write(\"\\n\")   \n",
    "        avg_val_loss = total_eval_loss / len(val_iter)\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        accuracy = 100*hits/total\n",
    "        unnormed = 100*words_unnormed/total\n",
    "        print(\"Epoch {} accuracy: \".format(epoch+1),accuracy)\n",
    "        print(unnormed)\n",
    "        Err_Red_rate = (accuracy - unnormed)/(100 - unnormed) # all are percentages\n",
    "        print(Err_Red_rate)\n",
    "        print(\"Epoch {} error reduction rate: \".format(epoch+1),Err_Red_rate)\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Error Reduction': Err_Red_rate,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    torch.save(model, 'token_classifier_'+name+'.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "step: 0, loss: 10.34687328338623\n",
      "Batch 500 of 7729.Elapsed: 0:02:28.\n",
      "Batch 1000 of 7729.Elapsed: 0:04:52.\n",
      "step: 1000, loss: 2.212703227996826\n",
      "Batch 1500 of 7729.Elapsed: 0:07:15.\n",
      "Batch 2000 of 7729.Elapsed: 0:09:42.\n",
      "step: 2000, loss: 2.44248366355896\n",
      "Batch 2500 of 7729.Elapsed: 0:12:04.\n",
      "Batch 3000 of 7729.Elapsed: 0:14:37.\n",
      "step: 3000, loss: 2.2548458576202393\n",
      "Batch 3500 of 7729.Elapsed: 0:17:09.\n",
      "Batch 4000 of 7729.Elapsed: 0:19:39.\n",
      "step: 4000, loss: 0.9408369064331055\n",
      "Batch 4500 of 7729.Elapsed: 0:22:16.\n",
      "Batch 5000 of 7729.Elapsed: 0:24:47.\n",
      "step: 5000, loss: 1.481183648109436\n",
      "Batch 5500 of 7729.Elapsed: 0:27:19.\n",
      "Batch 6000 of 7729.Elapsed: 0:29:46.\n",
      "step: 6000, loss: 1.1357345581054688\n",
      "Batch 6500 of 7729.Elapsed: 0:32:11.\n",
      "Batch 7000 of 7729.Elapsed: 0:34:37.\n",
      "step: 7000, loss: 1.7781896591186523\n",
      "Batch 7500 of 7729.Elapsed: 0:37:02.\n",
      "\n",
      "  Average training loss: 1.70\n",
      "  Training epoch took: 0:38:12\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:02:04\n",
      "Epoch 1 accuracy:  83.59020914574178\n",
      "24.676584067509854\n",
      "0.7821422375617462\n",
      "Epoch 1 error reduction rate:  0.7821422375617462\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "step: 0, loss: 1.0318106412887573\n",
      "Batch 500 of 7729.Elapsed: 0:02:38.\n",
      "Batch 1000 of 7729.Elapsed: 0:05:04.\n",
      "step: 1000, loss: 0.8624109029769897\n",
      "Batch 1500 of 7729.Elapsed: 0:07:31.\n",
      "Batch 2000 of 7729.Elapsed: 0:09:57.\n",
      "step: 2000, loss: 1.1830503940582275\n",
      "Batch 2500 of 7729.Elapsed: 0:12:22.\n",
      "Batch 3000 of 7729.Elapsed: 0:14:47.\n",
      "step: 3000, loss: 1.0622369050979614\n",
      "Batch 3500 of 7729.Elapsed: 0:17:10.\n",
      "Batch 4000 of 7729.Elapsed: 0:19:34.\n",
      "step: 4000, loss: 1.0147719383239746\n",
      "Batch 4500 of 7729.Elapsed: 0:21:56.\n",
      "Batch 5000 of 7729.Elapsed: 0:24:19.\n",
      "step: 5000, loss: 0.9412306547164917\n",
      "Batch 5500 of 7729.Elapsed: 0:26:41.\n",
      "Batch 6000 of 7729.Elapsed: 0:29:05.\n",
      "step: 6000, loss: 1.0281345844268799\n",
      "Batch 6500 of 7729.Elapsed: 0:31:29.\n",
      "Batch 7000 of 7729.Elapsed: 0:33:51.\n",
      "step: 7000, loss: 0.9039311408996582\n",
      "Batch 7500 of 7729.Elapsed: 0:36:12.\n",
      "\n",
      "  Average training loss: 0.93\n",
      "  Training epoch took: 0:37:19\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:01:58\n",
      "Epoch 2 accuracy:  86.1940426819623\n",
      "24.676584067509854\n",
      "0.8167109504113367\n",
      "Epoch 2 error reduction rate:  0.8167109504113367\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "step: 0, loss: 0.4552291929721832\n",
      "Batch 500 of 7729.Elapsed: 0:02:23.\n",
      "Batch 1000 of 7729.Elapsed: 0:04:44.\n",
      "step: 1000, loss: 0.4976744055747986\n",
      "Batch 1500 of 7729.Elapsed: 0:07:05.\n",
      "Batch 2000 of 7729.Elapsed: 0:09:27.\n",
      "step: 2000, loss: 1.5144078731536865\n",
      "Batch 2500 of 7729.Elapsed: 0:11:48.\n",
      "Batch 3000 of 7729.Elapsed: 0:14:11.\n",
      "step: 3000, loss: 0.6771427989006042\n",
      "Batch 3500 of 7729.Elapsed: 0:16:34.\n",
      "Batch 4000 of 7729.Elapsed: 0:18:56.\n",
      "step: 4000, loss: 0.3443881571292877\n",
      "Batch 4500 of 7729.Elapsed: 0:21:21.\n",
      "Batch 5000 of 7729.Elapsed: 0:23:44.\n",
      "step: 5000, loss: 0.2503783106803894\n",
      "Batch 5500 of 7729.Elapsed: 0:26:11.\n",
      "Batch 6000 of 7729.Elapsed: 0:28:34.\n",
      "step: 6000, loss: 1.1957169771194458\n",
      "Batch 6500 of 7729.Elapsed: 0:31:01.\n",
      "Batch 7000 of 7729.Elapsed: 0:33:32.\n",
      "step: 7000, loss: 0.6322697401046753\n",
      "Batch 7500 of 7729.Elapsed: 0:36:01.\n",
      "\n",
      "  Average training loss: 0.67\n",
      "  Training epoch took: 0:37:17\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 0.00\n",
      "  Validation took: 0:02:01\n",
      "Epoch 3 accuracy:  87.21535615298382\n",
      "24.676584067509854\n",
      "0.8302699938824517\n",
      "Epoch 3 error reduction rate:  0.8302699938824517\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:58:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "get_normalizer(archimob_corpus,'archimob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Normalization_Dataset \n",
    "import importlib\n",
    "NormalizationDataset = importlib.reload(Normalization_Dataset)\n",
    "from Normalization_Dataset import Normalization\n",
    "get_normalizer(wus_corpus,'wus')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
