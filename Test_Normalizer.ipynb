{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import importlib\n",
    "import normalizerFunctions \n",
    "normalizerFunctions = importlib.reload(normalizerFunctions)\n",
    "from normalizerFunctions import Training_Corpus, Token_Classifier\n",
    "from tokenize_GSW import tokenize, PUNCT\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from Testing_Dataset import Testing\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_test_sentence(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    f = torch.LongTensor\n",
    "    return words, f(x), is_heads, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_normalizations(model, iterable, idx2label):\n",
    "    normalized = []\n",
    "    model.eval()\n",
    "    for batch in iterable:\n",
    "        with torch.no_grad():\n",
    "            b_lines, x, b_is_heads, seqlens = batch\n",
    "            y = torch.zeros_like(x, dtype=torch.long)\n",
    "            _, _, b_predictions = model(x,y) # does not use y value\n",
    "            b_predictions= b_predictions.detach().cpu().numpy()\n",
    "            batch_norms = []\n",
    "            for line, line_preds, is_heads in zip(b_lines, b_predictions, b_is_heads):\n",
    "                line_preds = [pred for head, pred in zip(is_heads, line_preds) if head == 1]\n",
    "                for pred in line_preds:\n",
    "                    try:\n",
    "                        test = idx2label[pred]\n",
    "                    except KeyError:\n",
    "                        idx2label[pred] = '<pad>'\n",
    "                preds = [idx2label[pred] for pred in line_preds]\n",
    "                assert len(preds)==len(line)\n",
    "                batch_norms.append(preds[1:-1])\n",
    "            normalized.append(batch_norms)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus files\n",
    "with open('pickled_archimob.pkl', 'rb') as file:\n",
    "    archimob_corpus = pickle.load(file)\n",
    "with open('pickled_train_corpus.pkl', 'rb') as file:\n",
    "    joined_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-initialize model\n",
    "model = torch.load('token_classifier_archimob.pt')\n",
    "model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(True)\n",
    "    model = nn.DataParallel(model)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group utterances into lists of an arbitrary length so they are the same format as our songs\n",
    "def group_utterances(word_norm_pairs, groupsize=10):\n",
    "    grouped_list = []\n",
    "    sublist = []\n",
    "    for item in word_norm_pairs:\n",
    "        sublist.append(item)\n",
    "        if len(sublist) == groupsize:\n",
    "            grouped_list.append(sublist)\n",
    "            sublist = []\n",
    "    if sublist:\n",
    "        grouped_list.append(sublist)\n",
    "    return grouped_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_utterances =group_utterances(archimob_corpus.word_norm_pairs)\n",
    "utterances_unnormed = [[[word for word, _ in utterance] for utterance in utterances] for utterances in grouped_utterances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "archimob_dataset = Testing(utterances_unnormed)\n",
    "archimob_iter = DataLoader(dataset=archimob_dataset,\n",
    "                            batch_size=8,\n",
    "                            shuffle=False,\n",
    "                            num_workers=0,\n",
    "                            collate_fn=pad_test_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "archimob_normalized_by_classifier = get_normalizations(model, archimob_iter, model.idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Combined Model ERR for ArchiMob corpus\n",
    "    \n",
    "def get_model_norms_and_error_rate(list_of_utterances, model_predictions, counts):\n",
    "    normed_utterances = []\n",
    "    total = 0 \n",
    "    hits = 0\n",
    "    words_unnormed = 0\n",
    "    assert len(list_of_utterances)== len(model_predictions)\n",
    "    for u, p in zip(list_of_utterances,model_predictions):\n",
    "        normed_utterance = []\n",
    "        for (word, norm) , pred in zip(u, p):\n",
    "            if word == norm:\n",
    "                words_unnormed += 1\n",
    "            max_key = max(counts[word], key=counts[word].get) \n",
    "            if len(counts[word])>1:       # use the model's prediction unless there is only a single normalization\n",
    "                prediction = pred\n",
    "            else:\n",
    "                prediction = max_key\n",
    "            normed_utterance.append(prediction)\n",
    "            if prediction == norm:\n",
    "                hits += 1\n",
    "            total += 1\n",
    "        normed_utterances.append(normed_utterance)\n",
    "    accuracy = 100*hits/total\n",
    "    unnormed = 100*words_unnormed/total\n",
    "    print(accuracy)\n",
    "    print(unnormed)\n",
    "    Err_Red_rate = (accuracy - unnormed)/(100 - unnormed) \n",
    "    return normed_utterances, Err_Red_rate\n",
    "\n",
    "enhanced_norms, err = get_model_norms_and_error_rate(archimob_corpus.word_norm_pairs, \n",
    "                                                    [n_utterance for group in archimob_normalized_by_classifier for n_utterance in group],\n",
    "                                                    joined_corpus.norm_dict)\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_song_corpus(model, corpus_texts, idx2label):\n",
    "    corpus_dataset = Testing(corpus_texts)\n",
    "    index = 0\n",
    "    normalized_songs = [] \n",
    "    zipped = []\n",
    "    for song in corpus_texts:\n",
    "        song_len = len(song)\n",
    "        song_subset = data.Subset(corpus_dataset, range(index,index+song_len))\n",
    "        song_iter = DataLoader(dataset=song_subset,\n",
    "                                batch_size=512,\n",
    "                                shuffle=False,\n",
    "                                num_workers=0,\n",
    "                                collate_fn=pad_test_sentence)\n",
    "        normalized_by_classifier = get_normalizations(model, song_iter, idx2label)\n",
    "        normalized_songs.append(normalized_by_classifier)\n",
    "        zipped.append(zip(song,normalized_by_classifier))\n",
    "        index = index + song_len\n",
    "    return normalized_songs, zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus \n",
    "\n",
    "corpus = \"JSON_files/\"\n",
    "corpus_texts = []\n",
    "for filename in sorted(os.listdir(corpus)):\n",
    "    with open(corpus+filename) as file:\n",
    "        song_json = json.load(file)\n",
    "        lines = song_json[\"raw_text\"]\n",
    "        text = []\n",
    "        if len(lines[1].strip())>0:     # Ignore first two lines of songs with titles (songs with an empty second line):\n",
    "            start = 0\n",
    "        else:\n",
    "            start = 2\n",
    "        text = [tokenize(line, PUNCT) for line in lines[start:] if line]\n",
    "        corpus_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize corpus\n",
    "normalized_songs, zipped = normalize_song_corpus(model, corpus_texts,model.idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['es', 'ist', 'eine', 'mantel', 'gewesen', 'wo', 'sie', 'sich', 'dann', 'begegnet', 'sie'], ['sie', 'mit', 'rückblick', 'volle', 'vorher', 'an', 'seinem', 'haus', 'vorbei'], ['wo', 'er', 'das', 'lachen', 'sieht', 'ist', 'es', 'dann', 'plötzlich', 'um', 'ihn', 'gewesen'], ['er', 'lugen', 'nach', 'bis', 'er', 'sie', 'nicht', 'kann', 'sehen'], ['sie', 'ist', 'ein', 'blütenstecher', 'hat', 'wind', 'und', 'sonne', 'im', 'her', 'und', 'er', 'eine', 'jubel'], ['ein', 'mann', 'aus', 'der', 'bergen', 'nach', 'viel', 'jahre', 'sind', 'sie', 'iz', 'endlich', 'ein', 'paar'], ['weil', 'die', 'liebe', 'acker', 'ist', 'alles', 'mengen', 'schwer', 'willt', 'habe'], [], ['es', 'ist', 'es', 'schon', 'ein', 'wil', 'her', 'vergessen', 'hat', 'er', 'sie', 'nie', 'mehr'], ['was', 'wisch', 'dann', 'du', 'mit', 'so', 'vertrockneten', 'blütenstecher'], ['haben', 'die', 'leute', 'dann', 'hoch', 'genommen'], ['wenn', 'er', 'das', 'lachen', 'sieht', 'ist', 'gäng', 'noch', 'um', 'ihn', 'gewesen'], ['sie', 'herz', 'macht', 'luftschutz', 'wo', 'sie', 'auf', 'umhingewesen'], ['sie', 'ist', 'ein', 'blütenstecher', 'hat', 'wind', 'und', 'sonne', 'im', 'her', 'und', 'er', 'eine', 'jubel'], ['ein', 'mann', 'aus', 'der', 'bergen', 'nach', 'viel', 'jahre', 'sind', 'sie', 'iz', 'endlich', 'ein', 'paar'], ['weil', 'die', 'liebe', 'acker', 'ist', 'alles', 'mengen', 'schwer', 'willt', 'habe']]]\n"
     ]
    }
   ],
   "source": [
    "# flatten sublists\n",
    "normalized_songs = [song for [song] in normalized_songs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('normalized_songs.pickle', 'rb')\n",
    "old = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombinedModel(songs, model_predictions, counts):\n",
    "    total_corpus = 0\n",
    "    unnormed_corpus = 0\n",
    "    oov = 0 \n",
    "    ambiguous = 0\n",
    "    normed_songs = []\n",
    "    total_words = []\n",
    "    assert len(songs)== len(model_predictions)\n",
    "    for lines, preds in zip(songs,model_predictions):\n",
    "        normed_lines = []\n",
    "        for line, pred_line in zip(lines,preds):\n",
    "            normed_line = []\n",
    "            for word, pred in zip(line, pred_line):\n",
    "                if word in counts:\n",
    "                    max_key = max(counts[word], key=counts[word].get) \n",
    "                    if len(counts[word])>1:       # use the model's prediction unless there is only a single normalization\n",
    "                        prediction = pred\n",
    "                        ambiguous +=1 \n",
    "                    else:\n",
    "                        prediction = max_key\n",
    "                else:\n",
    "                    prediction = pred\n",
    "                    oov += 1\n",
    "                if prediction == word:\n",
    "                    unnormed_corpus += 1\n",
    "                normed_line.append(prediction)\n",
    "                total_words.append(prediction)\n",
    "                total_corpus+=1\n",
    "            normed_lines.append(normed_line)\n",
    "        normed_songs.append(normed_lines)\n",
    "    print(\"total tokens: \",len(total_words))\n",
    "    print(\"total unique tokens: \",len(set(total_words)))\n",
    "    print(\"proportion unique: \",len(set(total_words))/len(total_words))\n",
    "    print(\"total OOV: \",oov)\n",
    "    print(\"proportion\",oov/len(total_words))\n",
    "    print(\"proportion unnormed\",unnormed_corpus/len(total_words))\n",
    "    print(\"proportion normed by model: \", (ambiguous+oov)/len(total_words))\n",
    "    hapax = sum(1 for v in Counter(total_words).values() if v == 1)\n",
    "    print('hapax legomena: ',hapax)\n",
    "    return normed_songs, oov, unnormed_corpus, ambiguous, total_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFR_Model(songs, counts):\n",
    "    total_corpus = 0\n",
    "    unnormed_corpus = 0\n",
    "    oov = 0 \n",
    "    normed_songs = []\n",
    "    total_words = []\n",
    "    for song in songs:\n",
    "        normed_lines = []\n",
    "        for line in song:\n",
    "            normed_line = []\n",
    "            for word in line:\n",
    "                if word in counts:\n",
    "                    max_key = max(counts[word], key=counts[word].get) \n",
    "                    prediction = max_key \n",
    "                else:\n",
    "                    prediction = word\n",
    "                    oov += 1\n",
    "                if prediction == word:\n",
    "                    unnormed_corpus += 1\n",
    "                normed_line.append(prediction)\n",
    "                total_words.append(prediction)\n",
    "                total_corpus+=1\n",
    "            normed_lines.append(normed_line)\n",
    "        normed_songs.append(normed_lines)\n",
    "    print(\"total tokens: \",len(total_words))\n",
    "    print(\"total unique tokens: \",len(set(total_words)))\n",
    "    print(\"proportion unique: \",len(set(total_words))/len(total_words))\n",
    "    print(\"total OOV: \",oov)\n",
    "    print(\"proportion\",oov/len(total_words))\n",
    "    print(\"proportion unnormed\",unnormed_corpus/len(total_words))\n",
    "    hapax = sum(1 for v in Counter(total_words).values() if v == 1)\n",
    "    print('hapax legomena: ',hapax)\n",
    "    return normed_songs, oov, unnormed_corpus, total_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens:  52874\n",
      "total unique tokens:  8291\n",
      "proportion unique:  0.15680674811816772\n",
      "total OOV:  11031\n",
      "proportion 0.2086280591595113\n",
      "proportion unnormed 0.5908953360820063\n",
      "hapax legomena:  4927\n"
     ]
    }
   ],
   "source": [
    "normed_songs_MFR, _,_,_ = MFR_Model(corpus_texts,joined_corpus.norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tokens:  52874\n",
      "total unique tokens:  4047\n",
      "proportion unique:  0.07654045466580928\n",
      "total OOV:  11031\n",
      "proportion 0.2086280591595113\n",
      "proportion unnormed 0.36933086204940047\n",
      "proportion normed by model:  0.8887165714718008\n",
      "hapax legomena:  1406\n"
     ]
    }
   ],
   "source": [
    "normed_songs_combi_model, _,_,_,_ = CombinedModel(corpus_texts, normalized_songs, joined_corpus.norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_texts.pkl', \"wb\") as file:\n",
    "    pickle.dump(corpus_texts, file)\n",
    "with open('corpus_normalized.pkl', \"wb\") as file:\n",
    "    pickle.dump(normed_songs_combi_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
